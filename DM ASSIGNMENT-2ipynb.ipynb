{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import *\n",
    "import time\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "start_time = time.time()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data = pd.read_csv('rt_Reviews.csv', encoding='windows-1252')\n",
    "# Convert the Freshness column to binary labels\n",
    "data['Freshness'] = (data['Freshness'] == 'fresh').astype(int)\n",
    "# print(data.head())\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# Split the data into train, dev, and test sets\n",
    "train_data = data[:288000]\n",
    "# print(train_data.Freshness.value_counts())\n",
    "# print(train_data)\n",
    "dev_data = data[280000:384000]\n",
    "test_data = data[384000:]\n",
    "# print(train_data.shape)\n",
    "x_train = train_data['Review']\n",
    "y_train = train_data['Freshness']\n",
    "x_dev = dev_data['Review']\n",
    "y_dev = dev_data['Freshness']\n",
    "x_test = test_data['Review']\n",
    "y_test = test_data['Freshness']\n",
    "\n",
    "# print(x_train)\n",
    "\n",
    "#bbbb\n",
    "\n",
    "\n",
    "def get_vocabulary(dataset):\n",
    "    vocab_counts = defaultdict(int)\n",
    "    for review in dataset['Review']:\n",
    "        tokens = nltk.word_tokenize(review)\n",
    "        tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "        for token in tokens:\n",
    "            vocab_counts[token] += 1\n",
    "    vocab = [word for word, count in vocab_counts.items() if count >= 5]\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    return word_to_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_vocab = get_vocabulary(train_data)\n",
    "# print(train_vocab)\n",
    "\n",
    "# # Calculate the probability of each word occurring in a review\n",
    "def calc_word_occurrence_probability(word, dataset):\n",
    "    # Count the number of documents containing the given word\n",
    "    num_documents_with_word = sum(1 for review in dataset['Review'] if word in review.lower().split())\n",
    "\n",
    "    # Calculate the probability of occurrence of the word\n",
    "    total_num_documents = len(dataset)\n",
    "    p_word = num_documents_with_word / total_num_documents\n",
    "\n",
    "    return p_word\n",
    "p_good_train = calc_word_occurrence_probability(\"the\", train_data)\n",
    "# print(\"Probability of occurrence of 'good' in train dataset is {}\".format(p_good_train*100))\n",
    "\n",
    "\n",
    "def calc_word_sentiment_probability(word, sentiment, dataset):\n",
    "    # Count the number of documents containing the given word and sentiment\n",
    "    num_documents_with_word_and_sentiment = sum(1 for i, review in dataset.iterrows() if word in review['Review'].lower().split() and review['Freshness'] == sentiment)\n",
    "    # print(num_documents_with_word_and_sentiment)\n",
    "\n",
    "    # Count the number of documents containing the sentiment\n",
    "    num_documents_with_sentiment = sum(1 for i, review in dataset.iterrows() if review['Freshness'] == sentiment)\n",
    "    # print(num_documents_with_sentiment)\n",
    "\n",
    "\n",
    "    # Calculate the probability of occurrence of the word given the sentiment\n",
    "    p_word_given_sentiment = num_documents_with_word_and_sentiment / num_documents_with_sentiment\n",
    "\n",
    "    return p_word_given_sentiment\n",
    "p_the_positive_train = calc_word_sentiment_probability(\"good\", 1, train_data)\n",
    "print(\"Probability of 'the' given a positive sentiment in train dataset is {}\".format(p_the_positive_train*100))\n",
    "num_documents_with_sentiment = train_data['Freshness'].sum()\n",
    "p_positive = num_documents_with_sentiment / len(train_data)\n",
    "p_negative = 1 - p_positive\n",
    "\n",
    "\n",
    "\n",
    "def predict_review_sentiment(review, word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative):\n",
    "    # Convert the review to a list of tokens\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "\n",
    "    # Stem and remove stop words from the tokens\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Calculate the log probabilities of the review belonging to each class\n",
    "    log_p_positive = np.log(p_positive)\n",
    "    log_p_negative = np.log(p_negative)\n",
    "\n",
    "    # Sum the log probabilities of each word given each class\n",
    "    for token in stemmed_tokens:\n",
    "        if token in word_to_index:\n",
    "            token_index = word_to_index[token]\n",
    "            log_p_positive += np.log(positive_word_probs[token_index])\n",
    "            log_p_negative += np.log(negative_word_probs[token_index])\n",
    "\n",
    "    # Predict the sentiment with the higher probability\n",
    "    if log_p_positive > log_p_negative:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_accuracy(dataset, word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative):\n",
    "    num_correct = 0\n",
    "    for i, review in dataset.iterrows():\n",
    "        predicted_sentiment = predict_review_sentiment(review['Review'], word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative)\n",
    "        if predicted_sentiment == review['Freshness']:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_naive_bayes(x_train, y_train):\n",
    "    # Calculate the vocabulary of the training set\n",
    "    word_to_index = get_vocabulary(train_data)\n",
    "\n",
    "    # Calculate the probability of each word occurring in a review\n",
    "    positive_word_counts = np.zeros(len(word_to_index))\n",
    "    negative_word_counts = np.zeros(len(word_to_index))\n",
    "\n",
    "    for i, review in train_data.iterrows():\n",
    "        tokens = nltk.word_tokenize(review['Review'])\n",
    "        tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "        for token in tokens:\n",
    "            if token in word_to_index:\n",
    "                token_index = word_to_index[token]\n",
    "                if review['Freshness'] == 1:\n",
    "                    positive_word_counts[token_index] += 1\n",
    "                else:\n",
    "                    negative_word_counts[token_index] += 1\n",
    "\n",
    "    # Calculate the probability of each word given a positive or negative sentiment\n",
    "    num_documents_with_sentiment = train_data['Freshness'].sum()\n",
    "    p_positive = num_documents_with_sentiment / len(train_data)\n",
    "    p_negative = 1 - p_positive\n",
    "\n",
    "    positive_word_probs = (positive_word_counts + 1) / (positive_word_counts.sum() + len(word_to_index))\n",
    "    negative_word_probs = (negative_word_counts + 1) / (negative_word_counts.sum() + len(word_to_index))\n",
    "    # print('Positive word probabilities: {}'.format(positive_word_probs))\n",
    "    # print('Negative word probabilities: {}'.format(negative_word_probs))\n",
    "\n",
    "    return word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative\n",
    "\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative = train_naive_bayes(x_train, y_train)\n",
    "\n",
    "# Calculate the accuracy of the classifier on the development set\n",
    "accuracy = calculate_accuracy(dev_data, word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative)\n",
    "print('Accuracy on dev set: {:.2%}'.format(accuracy))\n",
    "\n",
    "def top_words(word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative):\n",
    "    top_positive_words = sorted(word_to_index.keys(), key=lambda w: positive_word_probs[word_to_index[w]] * p_positive / (positive_word_probs[word_to_index[w]] * p_positive + negative_word_probs[word_to_index[w]] * p_negative), reverse=True)[:10]\n",
    "    top_negative_words = sorted(word_to_index.keys(), key=lambda w: negative_word_probs[word_to_index[w]] * p_negative / (positive_word_probs[word_to_index[w]] * p_positive + negative_word_probs[word_to_index[w]] * p_negative), reverse=True)[:10]\n",
    "\n",
    "    return top_positive_words, top_negative_words\n",
    "\n",
    "top_positive_words, top_negative_words = top_words(word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative)\n",
    "\n",
    "print(\"Top 10 words predicting positive class:\")\n",
    "print(top_positive_words)\n",
    "\n",
    "print(\"Top 10 words predicting negative class:\")\n",
    "print(top_negative_words)\n",
    "\n",
    "\n",
    "def smoothings(x_train, y_train, smoothing):\n",
    "    # Count the number of positive and negative examples in the training set\n",
    "    n_positive = sum(1 for label in y_train if label == 1)\n",
    "    n_negative = len(y_train) - n_positive\n",
    "\n",
    "    # Calculate the prior probabilities P(positive) and P(negative)\n",
    "    p_positive = n_positive / len(y_train)\n",
    "    p_negative = 1 - p_positive\n",
    "\n",
    "    # Count the number of occurrences of each word in positive and negative examples\n",
    "    positive_word_counts = defaultdict(int)\n",
    "    negative_word_counts = defaultdict(int)\n",
    "    for words, label in zip(x_train, y_train):\n",
    "        for word in words:\n",
    "            if label == 1:\n",
    "                positive_word_counts[word] += 1\n",
    "            else:\n",
    "                negative_word_counts[word] += 1\n",
    "\n",
    "    # Create a dictionary that maps words to their index in the probability lists\n",
    "    all_words = set(positive_word_counts.keys()) | set(negative_word_counts.keys())\n",
    "    word_to_index = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    # Calculate the probability of each word given the positive and negative classes\n",
    "    num_words = len(all_words)\n",
    "    positive_word_probs = [(positive_word_counts[word] + smoothing) / (n_positive + smoothing * num_words) for word in\n",
    "                           all_words]\n",
    "    negative_word_probs = [(negative_word_counts[word] + smoothing) / (n_negative + smoothing * num_words) for word in\n",
    "                           all_words]\n",
    "\n",
    "    return word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative\n",
    "\n",
    "\n",
    "# Train a Naive Bayes classifier with Laplace smoothing\n",
    "word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative = smoothings(x_train, y_train,\n",
    "                                                                                             smoothing=1)\n",
    "\n",
    "# Calculate the accuracy of the classifier on the development set\n",
    "accuracy = calculate_accuracy(dev_data, word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative)\n",
    "print('Accuracy on dev set with smoothing=1: {:.2%}'.format(accuracy))\n",
    "\n",
    "# Train a Naive Bayes classifier with more aggressive smoothing\n",
    "word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative = smoothings(x_train, y_train,\n",
    "                                                                                             smoothing=10)\n",
    "\n",
    "# Calculate the accuracy of the classifier on the development set\n",
    "accuracy = calculate_accuracy(dev_data, word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative)\n",
    "print('Accuracy on dev set with smoothing=10: {:.2%}'.format(accuracy))\n",
    "\n",
    "# Train a Naive Bayes classifier with optimal hyperparameters\n",
    "word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative = smoothings(x_train, y_train,\n",
    "                                                                                             smoothing=0.5)\n",
    "accuracy = calculate_accuracy(dev_data, word_to_index, positive_word_probs, negative_word_probs, p_positive, p_negative)\n",
    "print('Accuracy on dev set with smoothing=0.5: {:.2%}'.format(accuracy))\n",
    "\n",
    "# Calculate the accuracy of the classifier on the test set\n",
    "accuracy = calculate_accuracy(test_data, word_to_index, positive_word_probs, negative_word_probs, p_positive,\n",
    "                              p_negative)\n",
    "print('Accuracy on test set with smoothing=1: {:.2%}'.format(accuracy))\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time taken: {:.2f} seconds'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
